{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a metadata file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ind = pd.read_csv('RSTC_2019_run_index_barcode - All.tsv', sep='\\t') # runs and indexes\n",
    "bac = pd.read_csv('RSTC_2019_run_index_barcode - Bac.tsv', sep='\\t') # bacterial samples\n",
    "bac.rename(columns={'Sample No.': 'Smpl_no'}, inplace=True)\n",
    "arc = pd.read_csv('RSTC_2019_run_index_barcode - Arc.tsv', sep='\\t') # archaea samples\n",
    "arc.rename(columns={'Sample No.': 'Smpl_no'}, inplace=True)\n",
    "met = pd.DataFrame()                                                 # metadata\n",
    "\n",
    "for ri in set(ind.Run_index): #add bacteria and archeae samples to metadata\n",
    "  tmp_ind = ind.loc[(ind.Run_index==ri)&(ind.Domain=='bac')].copy()  # bacteria\n",
    "  tmp = pd.merge(tmp_ind, bac, how='inner', on='Smpl_no')\n",
    "  met = pd.concat([met, tmp], ignore_index=True)\n",
    "  tmp_ind = ind.loc[(ind.Run_index==ri)&(ind.Domain=='arc')].copy()  # archaea\n",
    "  tmp = pd.merge(tmp_ind, arc, how='inner', on='Smpl_no')\n",
    "  met = pd.concat([met, tmp], ignore_index=True)\n",
    "\n",
    "met.rename(columns={'Mothur name':'#SampleID'}, inplace=True)        # rename sample id\n",
    "met.set_index('#SampleID', inplace=True)\n",
    "\n",
    "# add sequencing run information as separate column\n",
    "met = pd.read_csv('metadata.tsv', sep='\\t', index_col='#SampleID')\n",
    "met['SeqRun'] = ''\n",
    "for ri in set(met.Run_index):\n",
    "  for index, row in met.loc[met.Run_index==ri].copy().iterrows():\n",
    "    met.loc[index,'SeqRun'] = 'SeqRun_' + ri[1]\n",
    "    \n",
    "met.to_csv('metadata.tsv', sep='\\t')                                 # save a metadata file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare reads for import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'raw_data/LH_diet_2_r2_i6/MD5.txt': No such file or directory\n",
      "mv: cannot stat 'raw_data/LH_diet_2_r2_i6/*_1.fq.gz': No such file or directory\n",
      "mv: cannot stat 'raw_data/LH_diet_2_r2_i6/*_2.fq.gz': No such file or directory\n",
      "rm: cannot remove 'raw_data/LH_diet_2_r2_i9/MD5.txt': No such file or directory\n",
      "mv: cannot stat 'raw_data/LH_diet_2_r2_i9/*_1.fq.gz': No such file or directory\n",
      "mv: cannot stat 'raw_data/LH_diet_2_r2_i9/*_2.fq.gz': No such file or directory\n",
      "rm: cannot remove 'raw_data/LH_diet_2_r2_i5/MD5.txt': No such file or directory\n",
      "mv: cannot stat 'raw_data/LH_diet_2_r2_i5/*_1.fq.gz': No such file or directory\n",
      "mv: cannot stat 'raw_data/LH_diet_2_r2_i5/*_2.fq.gz': No such file or directory\n",
      "rm: cannot remove 'raw_data/LH_diet_2_r2_i3/MD5.txt': No such file or directory\n",
      "mv: cannot stat 'raw_data/LH_diet_2_r2_i3/*_1.fq.gz': No such file or directory\n",
      "mv: cannot stat 'raw_data/LH_diet_2_r2_i3/*_2.fq.gz': No such file or directory\n",
      "rm: cannot remove 'raw_data/LH_diet_2_r2_i1/MD5.txt': No such file or directory\n",
      "mv: cannot stat 'raw_data/LH_diet_2_r2_i1/*_1.fq.gz': No such file or directory\n",
      "mv: cannot stat 'raw_data/LH_diet_2_r2_i1/*_2.fq.gz': No such file or directory\n",
      "rm: cannot remove 'raw_data/LH_diet_2_r2_i4/MD5.txt': No such file or directory\n",
      "mv: cannot stat 'raw_data/LH_diet_2_r2_i4/*_1.fq.gz': No such file or directory\n",
      "mv: cannot stat 'raw_data/LH_diet_2_r2_i4/*_2.fq.gz': No such file or directory\n",
      "rm: cannot remove 'raw_data/LH_diet_2_r1_i12/MD5.txt': No such file or directory\n",
      "mv: cannot stat 'raw_data/LH_diet_2_r1_i12/*_1.fq.gz': No such file or directory\n",
      "mv: cannot stat 'raw_data/LH_diet_2_r1_i12/*_2.fq.gz': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "met = pd.read_csv('metadata.tsv', sep='\\t', index_col='#SampleID')\n",
    "for ri in set(met.Run_index):\n",
    "  path = 'raw_data/LH_diet_2_%s' % (ri)\n",
    "  !rm $path/MD5.txt                          # remove unecessary files\n",
    "  !mv $path/*_1.fq.gz $path/forward.fastq.gz # rename forward read for import\n",
    "  !mv $path/*_2.fq.gz $path/reverse.fastq.gz # rename reverse read for import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import raw reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiime2 import Artifact\n",
    "!mkdir -p Data/Imported_reads\n",
    "for ri in set(met.Run_index):\n",
    "  path = 'raw_data/LH_diet_2_%s' % ri\n",
    "  artf = Artifact.import_data('MultiplexedPairedEndBarcodeInSequence', path)\n",
    "  artf.save('Data/Imported_reads/%s_multiplexed.qza' % ri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demultiplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mSaved SampleData[PairedEndSequencesWithQuality] to: Data/Demuliplexed/r2_i6-demux.qza\u001b[0m\n",
      "\u001b[32mSaved MultiplexedPairedEndBarcodeInSequence to: Data/Demuliplexed/r2_i6-untrm.qza\u001b[0m\n",
      "\u001b[32mSaved SampleData[PairedEndSequencesWithQuality] to: Data/Demuliplexed/r2_i9-demux.qza\u001b[0m\n",
      "\u001b[32mSaved MultiplexedPairedEndBarcodeInSequence to: Data/Demuliplexed/r2_i9-untrm.qza\u001b[0m\n",
      "\u001b[32mSaved SampleData[PairedEndSequencesWithQuality] to: Data/Demuliplexed/r2_i5-demux.qza\u001b[0m\n",
      "\u001b[32mSaved MultiplexedPairedEndBarcodeInSequence to: Data/Demuliplexed/r2_i5-untrm.qza\u001b[0m\n",
      "\u001b[32mSaved SampleData[PairedEndSequencesWithQuality] to: Data/Demuliplexed/r2_i3-demux.qza\u001b[0m\n",
      "\u001b[32mSaved MultiplexedPairedEndBarcodeInSequence to: Data/Demuliplexed/r2_i3-untrm.qza\u001b[0m\n",
      "\u001b[32mSaved SampleData[PairedEndSequencesWithQuality] to: Data/Demuliplexed/r2_i1-demux.qza\u001b[0m\n",
      "\u001b[32mSaved MultiplexedPairedEndBarcodeInSequence to: Data/Demuliplexed/r2_i1-untrm.qza\u001b[0m\n",
      "\u001b[32mSaved SampleData[PairedEndSequencesWithQuality] to: Data/Demuliplexed/r2_i4-demux.qza\u001b[0m\n",
      "\u001b[32mSaved MultiplexedPairedEndBarcodeInSequence to: Data/Demuliplexed/r2_i4-untrm.qza\u001b[0m\n",
      "\u001b[32mSaved SampleData[PairedEndSequencesWithQuality] to: Data/Demuliplexed/r1_i12-demux.qza\u001b[0m\n",
      "\u001b[32mSaved MultiplexedPairedEndBarcodeInSequence to: Data/Demuliplexed/r1_i12-untrm.qza\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "met = pd.read_csv('metadata.tsv', sep='\\t', index_col='#SampleID')\n",
    "!mkdir -p Data/Demuliplexed\n",
    "for ri in set(met.Run_index):\n",
    "  metadata = met.loc[met.Run_index == ri]              # select by run and index\n",
    "  metadata.to_csv('metadata_%s.tsv'%ri, sep='\\t')      # save a temp metadata file\n",
    "  temp_met = 'metadata_%s.tsv'%ri                      # path to temp metadata\n",
    "  path  = 'Data/Imported_reads/%s_multiplexed.qza'%ri  # path to the multiplexed reads\n",
    "  demux = 'Data/Demuliplexed/%s-demux.qza'%ri          # demulptiplexed reads\n",
    "  untrm = 'Data/Demuliplexed/%s-untrm.qza'%ri          # untrimmed reads\n",
    "  \n",
    "  !qiime cutadapt demux-paired \\\n",
    "    --i-seqs $path \\\n",
    "    --m-forward-barcodes-file $temp_met \\\n",
    "    --m-forward-barcodes-column BarcodeSequence \\\n",
    "    --o-per-sample-sequences $demux \\\n",
    "    --o-untrimmed-sequences $untrm\n",
    "  !rm $temp_met"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate bacterial and archaea samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "met = pd.read_csv('metadata.tsv', sep='\\t', index_col='#SampleID')\n",
    "for ri in set(met.Run_index):\n",
    "  demux = 'Data/Demuliplexed/%s-demux.qza'%ri\n",
    "  a = !unzip $demux\n",
    "  digest = a[1].split('/')[0].replace('  inflating: ','')\n",
    "  \n",
    "  for f in os.listdir(digest+'/data'):\n",
    "    if f[0] == 'B':\n",
    "      !mkdir -p rstc_2019_bac/Data/Demux/$ri\n",
    "      !mv $digest/data/$f rstc_2019_bac/Data/Demux/$ri\n",
    "    if f[0] == 'A':\n",
    "      !mkdir -p rstc_2019_arc/Data/Demux/$ri\n",
    "      !mv $digest/data/$f rstc_2019_arc/Data/Demux/$ri\n",
    "      \n",
    "  !rm -r $digest\n",
    "!rm Data # remove processed intermediate files to save some space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional metadata manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "met = pd.read_csv('metadata.tsv', sep='\\t', index_col='#SampleID')\n",
    "met.drop(['Domain','Nr reads_0','Subsample'], axis=1, inplace=True)\n",
    "\n",
    "met.to_csv('metadata.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "  \n",
    "met = pd.read_csv('metadata.tsv', sep='\\t', index_col='#SampleID')\n",
    "met['BS'] = met['BodySite'].map(lambda a: ''.join([x[0].upper() for x in a.split(' ')])) # BodySite abbr.\n",
    "for ind in met.loc[met.BS=='L'].index:\n",
    "  met.loc[ind,'BS'] = 'SAM'\n",
    "\n",
    "dates = sorted(list(set(met['Date of Sampling'].tolist())),key=lambda d:tuple(map(int, d.split('/'))))\n",
    "tpdict = dict((j,i) for i,j in enumerate(dates))\n",
    "\n",
    "met['rstc_run'] = ''\n",
    "met['Day'] = ''\n",
    "met['Day_hour'] = ''\n",
    "met['Treatment'] = ''\n",
    "met['Source'] = ''\n",
    "for smpl in set(met['Sample Name']):\n",
    "  for index, row in met.loc[met['Sample Name']==smpl].copy().iterrows(): \n",
    "    for r in ['Run1','Run2']:                      # separate rstc run\n",
    "      if r in smpl:\n",
    "        met.loc[index,'rstc_run'] = 'rstc_' + r\n",
    "    for d in ['d0','d7','d13']:                    # separate day\n",
    "      if d in smpl:\n",
    "        met.loc[index,'Day'] = d               \n",
    "        if '48h' in smpl:                          # separate day and hours\n",
    "          met.loc[index,'Day_hour'] = d+'h48'\n",
    "        elif '24h' in smpl:\n",
    "          met.loc[index,'Day_hour'] = d+'h24'\n",
    "        else:\n",
    "          met.loc[index,'Day_hour'] = 'not_appl'\n",
    "    for t in range(1,6):                           # separate treatment\n",
    "      if 'Trt' not in smpl:\n",
    "        met.loc[index,'Treatment'] = 'not_appl'\n",
    "      if 'Trt'+str(t) in smpl:\n",
    "        met.loc[index,'Treatment'] = 'Trt'+str(t)        \n",
    "    for s in ['HP1','HP2','C1','C2','C3','mixed']: # source (heat pump, cow, pooled)\n",
    "      if s in smpl:\n",
    "        if s == 'mixed':\n",
    "          met.loc[index,'Source'] = 'mixC'\n",
    "        else:\n",
    "          met.loc[index,'Source'] = s\n",
    "met['Day_num'] = met.Day.str[1:].astype(int)\n",
    "\n",
    "# adding groups\n",
    "import itertools as it\n",
    "\n",
    "met['Src_rstcRun'] = met.Source + '_' + met.rstc_run\n",
    "cols = {'BS':'BS','Day':'Day','Day_hour':'Dh','Treatment':'Trt','Source':'Src'}\n",
    "groups = []\n",
    "for i,c in enumerate(cols):\n",
    "  if i != 0:\n",
    "    groups += list(it.combinations(cols,i+1))\n",
    "groups = [t for t in groups if len([x for x in t if 'Day' in x]) < 2 and 'BS' in t]\n",
    "\n",
    "for g in groups:\n",
    "  col = '_'.join([cols[c] for c in g])\n",
    "  met[col] = ''\n",
    "  for index,row in met.copy().iterrows():\n",
    "    met.loc[index,col] = '_'.join([met.at[index,c] for c in g])\n",
    "\n",
    "\n",
    "met.to_csv('metadata.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "met = pd.read_csv('metadata.tsv', sep='\\t', index_col='#SampleID')\n",
    "domains = {'A':'arc','B':'bac'}\n",
    "for d in domains:\n",
    "  met_d = met.loc[met.index.str[0] == d].copy()\n",
    "  met_d.to_csv('rstc_2019_%s/metadata.tsv'%domains[d], sep='\\t')  # separate and save a metadata files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
